---
title: "Time Series Analysis with R"
output: 
  flexdashboard::flex_dashboard:
    theme: united
    css: style.css
---

```{r knit_opts, include = FALSE}
#rm(list = ls()); gc()

knitr::opts_chunk$set(tidy       = FALSE,
                      cache      = FALSE,
                      message    = FALSE,
                      warning    = FALSE,
                      fig.height =     8,
                      fig.width  =    11)

library(conflicted)
library(tibble)
library(tidyverse)
library(dplyr)
library(stringr)
library(magrittr)
library(tidyquant)
library(ggplot2)
library(scales)
library(cowplot)
library(cranlogs)
library(timetk)
library(sweep)
library(seasonal)
library(forecast)
library(broom)

conflict_prefer('select',  'dplyr')
conflict_prefer('filter',  'dplyr')
conflict_prefer('lag',     'dplyr')
conflict_prefer('extract', 'magrittr')



options(width = 80L,
        warn  = 1
        )

set.seed(42)

theme_set(theme_cowplot())


source('custom_functions.R')
```

Simple Decomposition {data-navmenu="Decomposition"}
==========================================
column {.tabset}
---------------------------------------

### Additive Models

We start with the simplest decomposition: the simple additive model with a
moving average trend.



The `decompose` function in R allows us to build this:

```{r decompose_ap_simple_additive, echo=TRUE}
ap_ts_sa_decompose <- airpassengers_tbl %>%
  tk_ts(select = value, start = 1949, frequency = 12) %>%
  decompose()

ap_ts_sa_decompose %>% plot()
```

The package `sweep` provides us with a number of routines to help us tidy the
output of these routines.

```{r decompose_ap_simple_additive_tidy, echo=TRUE}
ap_ts_sa_decompose_tbl <- ap_ts_sa_decompose %>%
  sw_tidy_decomp(rename_index = 'month')


ap_ts_sa_decompose_tbl %>% glimpse()
ap_ts_sa_decompose_tbl %>% summary()
```

We have decomposed the time series into components, and the `sw_tidy_decomp()`
function calculates the 'seasonally adjusted' values of the trend i.e. the
'underlying' value ignoring seasonality.

Comparing the observed and seasonally adjusted values are straight-forward
from this, we plot both together.

```{r plot_passenger_sa_adjusted, echo=TRUE}
ggplot(ap_ts_sa_decompose_tbl) +
  geom_line(aes(x = month, y = observed)) +
  geom_line(aes(x = month, y = seasadj), colour = 'red') +
  expand_limits(y = 0) +
  xlab('Month') +
  ylab('Passenger Count') +
  scale_x_yearmon() +
  ggtitle('Comparison Lineplot of Passenger Data and Seasonal Counts',
          subtitle = '(seasonal adjustments in red)')

```


We now want to plot this data, and the easiest way to do this is to convert
it to 'long' format and plot each quantity separately.

```{r decompose_ap_sa_tidy_plot, echo=TRUE}
plot_tbl <- ap_ts_sa_decompose_tbl %>%
  gather('label', 'value', -month)

ggplot(plot_tbl) +
  geom_line(aes(x = month, y = value)) +
  facet_grid(rows = vars(label)) +
  expand_limits(y = 0) +
  xlab('Month') +
  ylab('Value') +
  scale_x_yearmon() +
  ggtitle('Decomposition Plot of the Simple Additive Model')
```

Due to the difference in scales for each parameter, we redo this plot but we
free up the $y$-axis scale.


```{r decompose_ap_sa_tidy_scales_plot, echo=TRUE}
ggplot(plot_tbl) +
  geom_line(aes(x = month, y = value)) +
  facet_grid(rows = vars(label), scales = 'free_y') +
  expand_limits(y = 0) +
  xlab('Month') +
  ylab('Value') +
  scale_x_yearmon() +
  ggtitle('Decomposition Plot of the Simple Additive Model')
```


#### Exercises

  1. Construct the simple additive decomposition on the Maine unemployment data.
  1. Construct the simple additive decomposition on the Australian CBE data.


### Multiplicative Models

Constructing the multiplicative decomposition model is similar, but this time
the seasonal component and error are multiplied to the trend rather than added.

Other than this, the multiplicative model works in a similar fashion. Once
again, while we focus on creating plots using `ggplot2` we will use the
default plotting system as a quick check.

```{r decompose_ap_simple_multiplicative, echo=TRUE}
ap_ts_sm_decompose <- airpassengers_tbl %>%
  tk_ts(select = value, start = 1949, frequency = 12) %>%
  decompose(type = 'multiplicative')

ap_ts_sm_decompose %>% plot()
```

Having created the decomposition object, we now inspect the outputs and produce
plots.

```{r decompose_ap_simple_multiplicative_tidy, echo=TRUE}
ap_ts_sm_decompose_tbl <- ap_ts_sm_decompose %>%
  sw_tidy_decomp(rename_index = 'month')

ap_ts_sm_decompose_tbl %>% glimpse()
ap_ts_sm_decompose_tbl %>% summary()
```

While the model is different from before, it produces similar outputs, and so
our plot code is almost identical to before.

We compare the observed values to the seasonally-adjusted ones first via a
lineplot.

```{r plot_passenger_sm_adjusted, echo=TRUE}
ggplot(ap_ts_sm_decompose_tbl) +
  geom_line(aes(x = month, y = observed)) +
  geom_line(aes(x = month, y = seasadj), colour = 'red') +
  expand_limits(y = 0) +
  xlab('Month') +
  ylab('Passenger Count') +
  scale_x_yearmon() +
  ggtitle('Comparison Lineplot of Passenger Data and Seasonal Counts (Multiplicative Mdodel)',
          subtitle = '(seasonal adjustments in red)')

```

We then look at the full decomposition plot, and each of the components.

```{r decompose_ap_sm_tidy_plot, echo=TRUE}
plot_tbl <- ap_ts_sm_decompose_tbl %>%
  gather('label', 'value', -month)

ggplot(plot_tbl) +
  geom_line(aes(x = month, y = value)) +
  facet_grid(rows = vars(label)) +
  expand_limits(y = 0) +
  xlab('Month') +
  ylab('Value') +
  scale_x_yearmon() +
  ggtitle('Decomposition Plot of the Simple Multiplicative Model for the Air Passenger Data')
```


Like for the additive model, the components of the decomposition are on
different scales, so we redo the plot with a free $y$-axis.

```{r decompose_ap_sm_tidy_scales_plot, echo=TRUE}
ggplot(plot_tbl) +
  geom_line(aes(x = month, y = value)) +
  facet_grid(rows = vars(label), scales = 'free_y') +
  expand_limits(y = 0) +
  xlab('Month') +
  ylab('Value') +
  scale_x_yearmon() +
  ggtitle('Decomposition Plot of the Simple Multiplicative Model for the Air Passenger Data')
```

#### Exercises

  1. Construct the simple multiplicative decomposition on the Maine
     unemployment data.
  1. Construct the simple multiplicative decomposition on the Australian CBE
     data.


### Drawbacks of Decomposition

The above approach for time-series decomposition have the benefit of
simplicity both in terms of intuition and calculation, but come with a number
of drawbacks:

  * Estimates of the trend are not available at the start and end of the
    time-series.
  * The trend-cycle estimate tends to over-smooth rapid rises and falls in the
    data - we can check for this by inspecting the remainder.
  * Seasonal components are fixed throughout the dataset and do not change - an
    assumption much too simplistic for real-world data, especially those
    collected over long periods of time
  * Idiosyncratic events causing short-term changes in behaviour are not
    managed well



STL Decomposition
=======================================
column
--------------------------------

<h4> Decompostion</h4>
The estimation of trend in a time series is often referred to as *smoothing* as
it tries to remove higher frequency noise in the signal to understand the
underlying signal.

These smoothing procedures often use data points both before and after the time
at which the smoothed estimate is to calculated. This makes it useless for
direct forecasting, but nevertheless can reveal interesting structure in the
data.

Previously we looked at using moving averages to estimate the trend, but
an alternative approach is to use *Seasonal and Trend decomposition using
Loess* or STL. This estimates the trend by using locally-weighted regression
(loess) methods.


### Periodic  STL

Our first use of STL involves using a regression window equal to the size of
the period of the data. For `AirPassengers`, this is 12, but we do not have
to set this explicitly.

```{r decompose_ap_stl_periodic, echo=TRUE}
ap_stl_decompose <- airpassengers_tbl %>%
  tk_ts(select = value, start = 1949, frequency = 12) %>%
  extract(,1) %>%       # need to use this for compatibility with stl()
  stl(s.window = 'periodic')

ap_stl_decompose %>% plot()
```

As before, we can use functions in the `sweep` package to create tidy outputs
to ease plotting and analysis.

```{r decompose_ap_stl_periodic_tidy, echo=TRUE}
ap_stl_decompose_tbl <- ap_stl_decompose %>%
  sw_tidy_decomp(rename_index = 'month')

ap_stl_decompose_tbl %>% glimpse()
ap_stl_decompose_tbl %>% summary()
```


This decomposition seems similar to the one produced by the `decompose()`
function, so we should compare the output. This is where tidy output is useful:
we use standard `dplyr` and `tidyr` methodologies to manipulate the data and
produce plots for visual inspection.

```{r create_decomposition_comparisons, echo=TRUE}
decomp_tbl <- ap_ts_sa_decompose_tbl %>%
  rename(remainder = random)

stl_tbl <- ap_stl_decompose_tbl


decomp_compare_tbl <- list(
  `Simple Additive` = decomp_tbl,
  `STL Periodic`    = stl_tbl
  ) %>%
  bind_rows(.id = 'category')

decomp_compare_tbl %>% glimpse()
decomp_compare_tbl %>% summary()
```

```{r plot_decomposition_seasonal, echo=TRUE}
ggplot(decomp_compare_tbl) +
  geom_line(aes(x = month, y = season, colour = category)) +
  expand_limits(y = 0) +
  xlab('Month') +
  ylab('Seasonal Value') +
  scale_x_yearmon() +
  ggtitle('Comparison Plot for Seasonal Values between Decompositional Methods')
```

Seasonal values are almost the same, so we now compare trend values.

```{r plot_decomposition_trends, echo=TRUE}
ggplot(decomp_compare_tbl) +
  geom_line(aes(x = month, y = trend, colour = category)) +
  expand_limits(y = 0) +
  xlab('Month') +
  ylab('Trend Value') +
  scale_x_yearmon() +
  ggtitle('Comparison Plot for Trends between Decompositional Methods')
```

We see here that the STL with Periodic smoothing window is almost the same as
the standard `decompose()` function for the Air Passenger data.


### Robust STL

We can also use robust loess to perform the STL, and compare the results.


```{r decompose_ap_stl_robust, echo=TRUE}
ap_stl_robust_decompose <- airpassengers_tbl %>%
  tk_ts(select = value, start = 1949, frequency = 12) %>%
  extract(,1) %>%       # need to use this for compatibility with stl()
  stl(s.window = 'periodic', robust = TRUE)

ap_stl_robust_decompose %>% plot()
```

As before, we can use functions in the `sweep` package to create tidy outputs
to ease plotting and analysis.

```{r decompose_ap_stl_robust_tidy, echo=TRUE}
ap_stl_robust_decompose_tbl <- ap_stl_robust_decompose %>%
  sw_tidy_decomp(rename_index = 'month')

ap_stl_robust_decompose_tbl %>% glimpse()
ap_stl_robust_decompose_tbl %>% summary()
```


### Decomposition Comparisons

We now directly compare all the decompositions.


```{r create_all_decomp_comparisons, echo=TRUE}
decomp_compare_tbl <- list(
  `Simple Additive` = decomp_tbl,
  `STL Periodic`    = stl_tbl,
  `STL Robust`      = ap_stl_robust_decompose_tbl
  ) %>%
  bind_rows(.id = 'category')

decomp_compare_tbl %>% glimpse()
decomp_compare_tbl %>% summary()
```

We have constructed the comparison dataset and now plot the seasonal values
for the different decompositions.

```{r plot_all_decomp_seasonal, echo=TRUE}
ggplot(decomp_compare_tbl) +
  geom_line(aes(x = month, y = season, colour = category)) +
  expand_limits(y = 0) +
  xlab('Month') +
  ylab('Seasonal Value') +
  scale_x_yearmon() +
  ggtitle('Comparison Plot for Seasonal Values between Decompositional Methods')
```

We now look at trend values for the different decompositions.


```{r plot_all_decomp_trends, echo=TRUE}
ggplot(decomp_compare_tbl) +
  geom_line(aes(x = month, y = trend, colour = category)) +
  expand_limits(y = 0) +
  xlab('Month') +
  ylab('Trend Value') +
  scale_x_yearmon() +
  ggtitle('Comparison Plot for Trends between Decompositional Methods')
```


Additional Decompositions
======================================================
column {.tabset}
-----------------------
Many more approaches to time-series decomposition exist, but a thorough
exploration of the techniques is beyond the scope of this workshop.
Furthermore, while a number of excellent packages exist in R to perform this
analysis, at the time of this workshop, they are not compatible with
`tidyquant`.

That said, some of these are very useful, so we will introduce them and show
some simple uses of them, and leave further exploration as an exercise.

### X11 Decomposition

The X11 method was developed by US Census Bureau and Statistics Canada to
assist them in the analysis of their statistical data collection and analysis.

Building upon the existing approaches discussed already, the approach allows
for the changes in seasonal effects over time, different day counts in each
month and other wrinkles such as holidays.

The `seasonal` package provides an implementation of this decomposition, and
while we do not have a `tidyquant` interface for it, the `forecast` package
provides some simple plotting routines.

```{r illustrate_x11_decomposition, echo=TRUE}
ap_x11_decompose <- airpassengers_tbl %>%
  tk_ts(select = value, start = 1949, frequency = 12) %>%
  seas(x11 = '')

ap_x11_decompose %>% plot()
ap_x11_decompose %>%
  autoplot() + ggtitle("Output of X11 Decomposition on Air Passenger Data")
```


### SEATS Decomposition

"SEATS" stands for *Seasonal Extraction in ARIMA Time Series*. Developed by the
Bank of Spain, this approach is commonly used by government agencies but only
works with quarterly and monthly data - data with seasonal effects at other
frequencies will require a different method.

As we did with X11, we will show the output for the default and leave its
exploration as an exercise.


```{r illustrate_seats_decomposition, echo=TRUE}
ap_seats_decompose <- airpassengers_tbl %>%
  tk_ts(select = value, start = 1949, frequency = 12) %>%
  seas()

ap_seats_decompose %>% plot()
ap_seats_decompose %>%
  autoplot() + ggtitle("Output of SEATS Decomposition on Air Passenger Data")
```



#### Time-Series Structure

Assuming we can remove the trend and the seasonal variation, we still have the
random component, $z_t$. Up to now, we have modelled this random component as
a sequence of random variables.

To simplify the analysis, we often make assumptions such like the random form
a sequence of *independent and identically distributed (i.i.d.)* random
variables, but this is rarely effective.

Most of the time, the $z_t$ are correlated and so we need to put some structure
on our framework for modelling this.

To that end, we now look at some statistical concepts and quantities that are
used to help with this modelling piece.


## Random Variables

The *expected value* or *expectation* of a random variable $x$, denoted $E(x)$,
is the mean value of $x$ in the population. Thus, for a continuous $x$, we have

$$
\mu = E(x) = \int p(x) \, x \, dx.
$$


and the *variance*, $\sigma^2$, is the expectation of the squared deviations,

$$
\text{Var}(x) = \sigma^2 = E[(x - \mu)^2],
$$

For bivariate data, each datapoint can be represented as $(x, y)$ and
we can generalise this concept to the *covariance*, $\gamma(x, y)$,

$$
\gamma(x, y) = \text{Cov}(x, y) =  E[(x - \mu_x)(y - \mu_y)].
$$


Correlation, $\rho$, is standardised covariance -- we scale the covariance by
the standard deviation of the two variables,

$$
\rho(x, y) = \frac{\gamma(x, y)}{\sigma_x \sigma_y}.
$$


The mean function of a time series model is

$$
\mu(t) = E(x_t),
$$

with the expectation being across the *ensemble* across histories of possible
time series produced by this model. In general, we only have one realisation of
this model, and so, without any further assumption, estimate the mean to be the
measured value.

If the mean function is constant, we say that the time-series is *stationary in
the mean*, and the estimate of the population mean is just the sample mean,

$$
\mu = \sum^n_{t=1} x_t.
$$


The variance function of a time-series model that is stationary in the
mean is given by

$$
\sigma^2(t) = E[(x_t - \mu)^2].
$$


If we make the further assumption that the time-series is also stationary in
the variance, then the population variance is just the sample variance:

$$
\text{Var}(x) = \frac{\sum(x_t - \mu)^2}{n - 1}
$$

### Exercises

  1. Are any of the time series we have analysed stationary in the mean?
  1. What about first differences?
  1. What about percentage changes?



## Autocorrelation (Serial Correlation)

Autocorrelation, also known as *serial correlation*, is the correlation between
random variables at different time intervals in a time series. We define the
*lag*, $k$ to be a fixed discrete interval for analysis. From this, we define
the *autocovariance function* and the *autocorrelation function* as functions
of $k$:

\begin{eqnarray*}
\gamma_k &=& E[(x_t - \mu)(x_{t+k} - \mu)], \\
\rho_k   &=& \frac{\gamma_k}{\sigma^2}.
\end{eqnarray*}


In R, the `acf()` function plots the *correlogram*, the plot of the sample
autocorrelation at $\rho_k$ against the lag $k$.


### Analysing the AirPassenger Data

We now turn our attention to calculating autocorrelations for a given time 
series, starting with the Air Passenger data.

Autocorrelations and the correlogram are one of the most important concepts and
tools in all of time series analysis, so while routines do exist to perform
the calculations and produce the plots, we will first go through the entire
process manually to ensure we have a thorough understanding of it all.

*NOTE:* Due to internal differences in how correlations are calculated
(`cor()` uses a different denominator to `acf()`) - our two methods will
produce slightly different values. In all cases, the values produced by `acf()`
and functions it depends on are the ones to trust.

Once we have gone through this process once, we will use the more direct
procedures.

```{r produce_airpassenger_lags, echo=TRUE}
ap_lags_tbl <- airpassengers_tbl %>%
  tq_mutate(
    select     = value,
    mutate_fun = lag.xts,
    k          = 1:24,
    col_rename = paste0('l', 1:24)
  ) %>%
  gather('lag_k','lag_val', -month, -value) %>%
  mutate(lag_k = lag_k %>% str_replace('^l', '') %>% as.integer())

ap_lags_tbl %>% glimpse()
ap_lags_tbl %>% summary()
```

To calculate the autocorrelations at various lags we group by the lag amount,
calculate the correlation where we have values (we do not want to include the
NA values in the calculation).

Note that even independent pairs of numbers will be likely to calculate a small
amount of correlation due to sample noise. This threshold can be shown to be

$$
\text{Threshold} = \frac{2}{\sqrt{N}}
$$

We include these cutoff bounds in the calculation.

```{r calculate_ap_correlations, echo=TRUE}
ap_autocorrs_tbl <- ap_lags_tbl %>%
  group_by(lag_k) %>%
  summarise(
    cor          = cor(x = value, y = lag_val, use = "pairwise.complete.obs"),
    cutoff_upper =  2 / sqrt(n()),
    cutoff_lower = -2 / sqrt(n())
  )

ap_autocorrs_tbl %>% glimpse()
ap_autocorrs_tbl %>% summary()
```

Now that we have the data in the appropriate form, it is straight-forward to
construct a plot in `ggplot2` to visualise the data.

```{r show_ap_correlogram, echo=TRUE}
ggplot(ap_autocorrs_tbl) +
  geom_line(aes(x = lag_k, y = cor)) +
  geom_line(aes(x = lag_k, y = cutoff_upper), colour = 'blue', linetype = 'dashed') +
  geom_line(aes(x = lag_k, y = cutoff_lower), colour = 'blue', linetype = 'dashed') +
  xlab('Lag') +
  ylab('Auto-correlation') +
  ggtitle('Correlogram for the Air Passenger Data')
```

As mentioned above, we can calculate these values directly in R by using
`acf()` and its more user-friendly function from `forecast`, `Acf()`.

```{r calculate_ap_acf, echo=TRUE}
ap_acf <- airpassengers_tbl %>%
  tk_ts(select = value, start = 1949, frequency = 12) %>%
  Acf()

ap_acf %>% autoplot() + ggtitle('Correlogram of the Air Passenger Data')
```


### Partial Autocorrelation

The correlogram is useful for checking correlations lags, but due to the nature
of the calculation, the autocorrelation at higher lags is related to those at
lower lags.

For this reason, we often look at *partial autocorrelations* - where the
correlation due to earlier lags is removed. The R code for calculating the
PACF is almost identical as for the ACF.

```{r calculate_ap_pacf, echo=TRUE}
ap_pacf <- airpassengers_tbl %>%
  tk_ts(select = value, start = 1949, frequency = 12) %>%
  Pacf()

ap_pacf %>% autoplot() + ggtitle('Partial Correlogram of the Air Passenger Data')
```

We see the two main correlations are at lag 1 and lag 13, manifesting the
strong annual seasonality in the data.

Partial autocorrelations are especially useful for modelling autoregressive
models, and we will return to this later.

## Multivariate Covariance

Multivariate series has a temporal equivalent to correlation and covariance,
known as the *cross-covariance function (ccvf)* and the *cross-correlation
function (ccf)*,

\begin{eqnarray}
\gamma_k(x, y) &=& E[(x_{t+k} - \mu_x)(y_t - \mu_y)], \\
\rho_k(x, y)   &=& \frac{\gamma_k(x, y)}{\sigma_x \sigma_y}.
\end{eqnarray}

Note that the above functions are not symmetric, as the lag is always
on the first variable, $x$.

A corollary of this definition is that negative lags switch the order of
$(x, y)$

$$
\gamma_k(x, y) = \gamma_{-k}(y, x)
$$


### Building Activity

Before we look at the data, we might expect to have a good example of leading
and lagging variables in the Australian Building data: one of the values is
the number of construction approvals, the other is the amount of activity.

With this data we might expect that the behaviour in approvals will reflect
later behaviour in activity - there is a natural chain of causality there.

So, we now have something to investigate, and the code functionality we know
can extend to multivariate time series.


```{r show_cross_correlation_building, echo=TRUE}
approvactiv_cross_acf <- approvactiv_ts %>%
  Acf(plot = FALSE)

approvactiv_cross_acf %>%
  autoplot() +
    ggtitle('Cross-correlations of the Australian Building Data')
```

Not that these plots are not symmetric: the plots on the diagonals give us
the univariate correlograms, where as plot (2,1) on the bottom left shows us
the cross-correlation of approvals to activity, i.e. whether or not the
approval values lead activity values.

We see quite a few lags are above the 'noise' threshold, indicating there is a
leading effect.

The plot is the reverse: do activity values lead approval values. Here we see
little evidence of this.


### Cross-Correlation and Non-Stationary Series

In general, cross-correlation is defined for stationary time series as the
for non-stationary series may have trends and seasonal effects, these may
dominate the calculation. As a result, we typically remove trends and
seasonality before looking for cross-correlations.

To illustrate this, we do a simple exercise of producing independent series
with trends and seasonality and calculate cross-correlations.

```{r check_crosscorrs_trend, echo=TRUE}
produce_trend_series <- function(strength, trend = 1:100, N = 100) {
  x <- trend + strength * rnorm(N)
  y <- trend + strength * rnorm(N)
  
  data_tbl <- tibble(
    i = seq_along(trend),
    x = x,
    y = y
  )
  
  return(data_tbl)
}

```

```{r}
gen_data_tbl <- tibble(strength = c(1, 10, 100, 1000)) %>%
  mutate(data = map(strength, produce_trend_series)) %>%
  unnest()

ggplot(gen_data_tbl) +
  geom_line(aes(x = i, y = x)) +
  geom_line(aes(x = i, y = y), colour = 'red') +
  facet_wrap(~strength, ncol = 2, scale = 'free_y')

```

Having looked at the series, we see that for low values of `strength` the trend
dominates the value. We now construct the cross-correlation function for each
of these bivariate series.

```{r plot_bivariate_trend_ccf, echo=TRUE}
create_ccf_plot <- function(ccf_data, strength) {
  autoplot(ccf_data) +
    ggtitle(paste('Strength', strength))
}

gen_data_tbl %>%
  group_by(strength) %>%
  summarise(ccf_data = list(Ccf(x, y, plot = FALSE))) %>%
  mutate(ccf_plot = map2(ccf_data, strength, create_ccf_plot)) %>%
  pull(ccf_plot) %>%
  plot_grid(plotlist = ., ncol = 2)
```

As we see, we pick up a lot of cross-correlation with a strong trend, but this
drops off for the series with weaker trends.

Similarly, we look at seasonal data, constructing some toy bivariate series
with seasonal effects and then look at the cross-correlations.

```{r construct_bivariate_seasonal, echo=TRUE}
produce_seasonal_series <- function(strength, offset = 4, N = 370) {
  time <- 0:N

  x <- sin(2 * pi * time / 37)            + strength * rnorm(N+1)
  y <- sin(2 * pi * (time + offset) / 37) + strength * rnorm(N+1)
  
  data_tbl <- tibble(
    i = seq_along(time),
    x = x,
    y = y
  )
  
  return(data_tbl)
}

gen_data_tbl <- tibble(strength = c(0, 0.1, 0.5, 1, 2, 10)) %>%
  mutate(data = map(strength, produce_seasonal_series)) %>%
  unnest()

ggplot(gen_data_tbl) +
  geom_line(aes(x = i, y = x)) +
  geom_line(aes(x = i, y = y), colour = 'red') +
  facet_wrap(~ strength, ncol = 3, scale = 'free_y')
```



```{r plot_bivariate_seasonal_ccf, echo=TRUE}
create_ccf_plot <- function(ccf_data, strength) {
  autoplot(ccf_data) + ggtitle(paste('Strength', strength))
}

gen_data_tbl %>%
  group_by(strength) %>%
  summarise(ccf_data = list(Ccf(x, y, plot = FALSE))) %>%
  mutate(ccf_plot = map2(ccf_data, strength, create_ccf_plot)) %>%
  pull(ccf_plot) %>%
  plot_grid(plotlist = ., ncol = 3)
```



# Basic Forecasting

The basic task of forecasting is the prediction of future values for some
quantity.

One effective method of forecasting is to find a related variable whose value
leads it by one or more timesteps. This related variable is often called a
*leading indicator*.

A good example of a leading indicator is the ADP numbers in the US. ADP is the
main provider of payroll software in the US, and it releases information in
advance of US Unemployment statistics. Economic and financial analysts pay
close attention to this quantity as it is predictive of the unemployment
numbers.

In quantitative trading, one of the first popular and profitable uses of
statistical and time series analysis was in *pairs trading* - closely related
stocks tend to move together and this analysis enabled profitable trades.

So, for any given quantity, the trick is to find a leading variable and this is
often unreliable or non-existent. As a result, we start with the task of using
univariate analysis to make our predictions.


